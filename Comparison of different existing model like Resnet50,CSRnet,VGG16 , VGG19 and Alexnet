# ==============================================================================
# Crowd Counting Model Comparison (Fixed 50 Epochs & Separate Plots)
#
# This script trains and evaluates five different deep learning architectures
# for the task of crowd counting on the Mall dataset. All models are trained
# for a fixed 50 epochs.
#
# Finally, it generates individual plots for loss and MAE for each model.
# ==============================================================================

# --- 1. Imports and Setup ---
import os
import kagglehub
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import mean_absolute_error, mean_squared_error

# TensorFlow and Keras Imports
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten, Dropout, Activation, GlobalAveragePooling2D
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import resnet50, vgg16, vgg19

# Set plotting style and seed for reproducibility
sns.set(style='whitegrid', context='notebook', palette='deep')
np.random.seed(42)
tf.random.set_seed(42)

# --- 2. Data Preparation ---

# --- 2.1 Download and Load Data ---
print("Downloading dataset from Kaggle Hub...")
try:
    path = kagglehub.dataset_download("fmena14/crowd-counting")
    print(f"Dataset downloaded to: {path}")
except Exception as e:
    print(f"An error occurred during download. Please ensure you are logged into Kaggle.")
    print(f"Error: {e}")
    exit()

# --- 2.2 Define Paths and Load Labels ---
data_dir = os.path.join(path, 'frames', 'frames')
labels_path = os.path.join(path, 'labels.csv')

df = pd.read_csv(labels_path)
df['image_name'] = df['id'].map('seq_{:06d}.jpg'.format)
df = df.sort_values('id').reset_index(drop=True)

print("\nData loaded and sorted:")
print(df.head())

# --- 2.3 Setup ImageDataGenerator ---
IMG_SIZE = 224
BATCH_SIZE = 32
EPOCHS = 50

datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2
)

flow_params = dict(
    dataframe=df,
    directory=data_dir,
    x_col="image_name",
    y_col="count",
    target_size=(IMG_SIZE, IMG_SIZE),
    color_mode='rgb',
    class_mode="raw",
    batch_size=BATCH_SIZE,
    seed=42
)

train_generator = datagen.flow_from_dataframe(
    subset='training',
    shuffle=True,
    **flow_params
)

valid_generator = datagen.flow_from_dataframe(
    subset='validation',
    shuffle=False,
    **flow_params
)

print(f"\nFound {train_generator.n} images for training.")
print(f"Found {valid_generator.n} images for validation.")


# --- 3. Model Definitions --- (Functions are identical)

def build_keras_app_model(app_model, model_name):
    base_model = app_model(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3), pooling='avg')
    base_model.trainable = False
    x = base_model.output
    x = Dense(512, activation='relu')(x)
    x = Dropout(0.5)(x)
    predictions = Dense(1, activation='linear')(x)
    model = Model(inputs=base_model.input, outputs=predictions, name=model_name)
    return model

def build_alexnet_model():
    model = tf.keras.models.Sequential([
        Conv2D(96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)),
        MaxPooling2D(pool_size=(3,3), strides=(2,2)),
        Conv2D(256, kernel_size=(5,5), padding='same', activation='relu'),
        MaxPooling2D(pool_size=(3,3), strides=(2,2)),
        Conv2D(384, kernel_size=(3,3), padding='same', activation='relu'),
        Conv2D(384, kernel_size=(3,3), padding='same', activation='relu'),
        Conv2D(256, kernel_size=(3,3), padding='same', activation='relu'),
        MaxPooling2D(pool_size=(3,3), strides=(2,2)),
        Flatten(),
        Dense(4096, activation='relu'), Dropout(0.5),
        Dense(4096, activation='relu'), Dropout(0.5),
        Dense(1, activation='linear')
    ], name="AlexNet")
    return model

def build_csrnet_model():
    vgg16_base = vgg16.VGG16(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))
    frontend = Model(inputs=vgg16_base.input, outputs=vgg16_base.get_layer('block5_conv3').output)
    frontend.trainable = False
    backend = frontend.output
    backend = Conv2D(512, (3, 3), padding='same', dilation_rate=2, activation='relu')(backend)
    backend = Conv2D(512, (3, 3), padding='same', dilation_rate=2, activation='relu')(backend)
    backend = Conv2D(256, (3, 3), padding='same', dilation_rate=2, activation='relu')(backend)
    backend = Conv2D(128, (3, 3), padding='same', dilation_rate=2, activation='relu')(backend)
    backend = Conv2D(64, (3, 3), padding='same', dilation_rate=2, activation='relu')(backend)
    density_map = Conv2D(1, (1, 1), activation='relu', name='density_map')(backend)
    count_output = GlobalAveragePooling2D()(density_map)
    model = Model(inputs=frontend.input, outputs=count_output, name="CSRNet_Adapted")
    return model


# --- 4. Training and Evaluation Loop ---

models_to_compare = {
    "ResNet50": build_keras_app_model(resnet50.ResNet50, "ResNet50"),
    "VGG16": build_keras_app_model(vgg16.VGG16, "VGG16"),
    "VGG19": build_keras_app_model(vgg19.VGG19, "VGG19"),
    "AlexNet": build_alexnet_model(),
    "CSRNet": build_csrnet_model()
}

results = []
histories = {}

for model_name, model in models_to_compare.items():
    print("\n" + "="*50)
    print(f"--- Training Model: {model_name} for 50 Epochs ---")
    print("="*50)

    model.compile(
        optimizer=Adam(learning_rate=0.0001),
        loss='mean_squared_error',
        metrics=['mean_absolute_error']
    )

    history = model.fit(
        train_generator,
        epochs=EPOCHS,
        validation_data=valid_generator,
        verbose=1
    )
    histories[model_name] = history

    print(f"\n--- Evaluating {model_name} after 50 Epochs ---")
    valid_generator.reset()

    predictions = model.predict(valid_generator)
    validation_df = df.iloc[valid_generator.index_array]
    true_counts = validation_df['count'].values

    mae = mean_absolute_error(true_counts, predictions)
    mse = mean_squared_error(true_counts, predictions)

    results.append({
        "Model": model_name, "MAE": mae, "MSE": mse, "Parameters": model.count_params()
    })

    print(f"Evaluation for {model_name}: MAE={mae:.4f}, MSE={mse:.4f}")


# --- 5. Final Results Summary ---
print("\n" + "="*60)
print("--- Final Model Comparison Summary (Full 50 Epochs) ---")
print("="*60)

results_df = pd.DataFrame(results)
results_df['Parameters'] = results_df['Parameters'].apply(lambda x: f"{x:,}")
print(results_df.to_string(index=False))


# --- 6. Generate Separate Performance Graphs for Each Model ---
print("\n" + "="*60)
print("--- Generating Individual Model Performance Graphs ---")
print("="*60)

for model_name, history in histories.items():
    # Create a figure with two subplots (one for loss, one for MAE)
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 12))
    fig.suptitle(f'Performance Graphs for {model_name}', fontsize=16)

    # Plot Training & Validation Loss
    ax1.plot(history.history['loss'], label='Training Loss')
    ax1.plot(history.history['val_loss'], label='Validation Loss')
    ax1.set_title('Model Loss (MSE)')
    ax1.set_ylabel('Loss')
    ax1.set_xlabel('Epoch')
    ax1.legend(loc='upper right')
    ax1.grid(True)

    # Plot Training & Validation MAE (Accuracy)
    ax2.plot(history.history['mean_absolute_error'], label='Training MAE')
    ax2.plot(history.history['val_mean_absolute_error'], label='Validation MAE')
    ax2.set_title('Model "Accuracy" (Mean Absolute Error)')
    ax2.set_ylabel('Mean Absolute Error')
    ax2.set_xlabel('Epoch')
    ax2.legend(loc='upper right')
    ax2.grid(True)
    
    # Adjust layout and display the plots for the current model
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.show()
